"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[5086],{2527:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module-2-gazebo-unity/sensors","title":"Sensor Simulation","description":"1. Introduction to Sensor Simulation","source":"@site/docs/module-2-gazebo-unity/sensors.md","sourceDirName":"module-2-gazebo-unity","slug":"/module-2-gazebo-unity/sensors","permalink":"/physical-ai-book/docs/module-2-gazebo-unity/sensors","draft":false,"unlisted":false,"editUrl":"https://github.com/RukhsarMalik/physical-ai-book/tree/main/docs/module-2-gazebo-unity/sensors.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"sensors","title":"Sensor Simulation","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Gazebo Fundamentals","permalink":"/physical-ai-book/docs/module-2-gazebo-unity/gazebo-basics"},"next":{"title":"Unity for Robotics","permalink":"/physical-ai-book/docs/module-2-gazebo-unity/unity-integration"}}');var i=s(4848),a=s(8453);const o={id:"sensors",title:"Sensor Simulation",sidebar_position:3},t=void 0,l={},c=[{value:"1. Introduction to Sensor Simulation",id:"1-introduction-to-sensor-simulation",level:2},{value:"2. LiDAR Simulation",id:"2-lidar-simulation",level:2},{value:"3. Depth Camera Simulation",id:"3-depth-camera-simulation",level:2},{value:"4. IMU Sensors",id:"4-imu-sensors",level:2},{value:"5. Reading Sensor Data in ROS 2",id:"5-reading-sensor-data-in-ros-2",level:2}];function d(e){const n={code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"1-introduction-to-sensor-simulation",children:"1. Introduction to Sensor Simulation"}),"\n",(0,i.jsxs)(n.p,{children:["Robots perceive their environment primarily through ",(0,i.jsx)(n.strong,{children:"sensors"}),". For Physical AI, accurate sensor data is paramount for tasks like navigation, object recognition, manipulation, and decision-making. In simulation, we don't have physical sensors, so we rely on ",(0,i.jsx)(n.strong,{children:"simulated sensors"})," that mimic the behavior and data output of their real-world counterparts."]}),"\n",(0,i.jsx)(n.p,{children:"Gazebo provides a rich set of built-in simulated sensors, allowing you to attach virtual LiDARs, cameras, IMUs, sonars, and more to your robot models. These simulated sensors publish data in ROS 2 message formats, making them seamlessly compatible with actual robot control and AI algorithms developed in ROS 2."}),"\n",(0,i.jsx)(n.p,{children:"The benefits of sensor simulation include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cost-Effectiveness"}),": No need to buy expensive physical sensors for initial development and testing."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety"}),": Test algorithms in hazardous or extreme conditions without risk to physical hardware."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reproducibility"}),": Generate identical sensor data streams repeatedly for robust debugging and algorithm evaluation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Parameter Tuning"}),": Easily change sensor parameters (e.g., LiDAR range, camera resolution) to understand their impact."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ground Truth Access"}),': In simulation, you have access to the "ground truth" (e.g., exact object positions, velocities), which is invaluable for debugging perception algorithms and generating training data for AI.']}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"2-lidar-simulation",children:"2. LiDAR Simulation"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"LiDAR (Light Detection and Ranging)"})," sensors are crucial for robotic navigation and mapping. They generate a point cloud, which is a set of data points in a 3D coordinate system, representing the environment's shape."]}),"\n",(0,i.jsx)(n.p,{children:'In Gazebo, a simulated LiDAR (often referred to as a "Ray Sensor" or "GPU Ray Sensor" for performance) can be configured within your robot\'s SDF/URDF file.'}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key parameters to configure for a simulated LiDAR:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:[(0,i.jsx)(n.code,{children:"ray"})," element"]}),": Defines the sensor's characteristics.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"scan"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"horizontal"})," and ",(0,i.jsx)(n.code,{children:"vertical"}),": Specify the angle range, resolution (number of rays), and maximum range."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"range"}),": Defines minimum, maximum, and resolution of a single ray."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"always_on"})}),": Whether the sensor is always active."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"update_rate"})}),": How often the sensor publishes data (Hz)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"visualize"})}),": Whether to visualize the rays in Gazebo GUI."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"plugin"})}),": A ROS 2 Gazebo plugin (e.g., ",(0,i.jsx)(n.code,{children:"libgazebo_ros_laser.so"}),") to bridge the simulated data to ROS 2 topics, typically publishing ",(0,i.jsx)(n.code,{children:"sensor_msgs/msg/LaserScan"})," or ",(0,i.jsx)(n.code,{children:"sensor_msgs/msg/PointCloud2"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example (SDF Snippet for a LiDAR Sensor):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<sensor name="laser_sensor" type="ray">\r\n  <pose>0.1 0 0.2 0 0 0</pose> \x3c!-- Position relative to its parent link --\x3e\r\n  <visualize>true</visualize>\r\n  <update_rate>10</update_rate> \x3c!-- 10 Hz --\x3e\r\n  <ray>\r\n    <scan>\r\n      <horizontal>\r\n        <samples>720</samples>      \x3c!-- Number of rays --\x3e\r\n        <resolution>1</resolution>  \x3c!-- Resolution factor --\x3e\r\n        <min_angle>-1.5708</min_angle> \x3c!-- -90 degrees --\x3e\r\n        <max_angle>1.5708</max_angle>  \x3c!-- +90 degrees --\x3e\r\n      </horizontal>\r\n    </scan>\r\n    <range>\r\n      <min>0.1</min>\r\n      <max>10.0</max>\r\n      <resolution>0.01</resolution>\r\n    </range>\r\n  </ray>\r\n  <plugin name="laser_controller" filename="libgazebo_ros_laser.so">\r\n    <topicName>scan</topicName>\r\n    <frameName>laser_frame</frameName>\r\n  </plugin>\r\n</sensor>\n'})}),"\n",(0,i.jsx)(n.h2,{id:"3-depth-camera-simulation",children:"3. Depth Camera Simulation"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Depth cameras"})," (like Intel RealSense or Microsoft Kinect) provide both a standard color image and a depth map, which represents the distance of objects from the camera. This 3D information is critical for tasks like object pose estimation, obstacle avoidance, and 3D mapping."]}),"\n",(0,i.jsxs)(n.p,{children:["In Gazebo, a simulated depth camera (often ",(0,i.jsx)(n.code,{children:"depth_camera"})," type or a ",(0,i.jsx)(n.code,{children:"camera"})," with depth configuration) can also be defined in your model's SDF/URDF."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:[(0,i.jsx)(n.code,{children:"camera"})," element"]}),": Defines optical properties.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"horizontal_fov"}),", ",(0,i.jsx)(n.code,{children:"image"})," (width, height, format), ",(0,i.jsx)(n.code,{children:"clip"})," (near, far)."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"plugin"})}),": A ROS 2 Gazebo plugin (e.g., ",(0,i.jsx)(n.code,{children:"libgazebo_ros_depth_camera.so"}),") to publish data to ROS 2 topics, typically:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"sensor_msgs/msg/Image"})," (color image)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"sensor_msgs/msg/Image"})," (depth image)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"sensor_msgs/msg/PointCloud2"})," (3D point cloud)"]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"sensor_msgs/msg/CameraInfo"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example (SDF Snippet for a Depth Camera):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<sensor name="depth_camera_sensor" type="depth_camera">\r\n  <pose>0 0 0 0 0 0</pose>\r\n  <visualize>true</visualize>\r\n  <update_rate>30.0</update_rate> \x3c!-- 30 Hz --\x3e\r\n  <camera>\r\n    <horizontal_fov>1.047</horizontal_fov> \x3c!-- 60 degrees --\x3e\r\n    <image>\r\n      <width>640</width>\r\n      <height>480</height>\r\n      <format>R8G8B8</format>\r\n    </image>\r\n    <clip>\r\n      <near>0.1</near>\r\n      <far>10</far>\r\n    </clip>\r\n  </camera>\r\n  <plugin name="camera_controller" filename="libgazebo_ros_depth_camera.so">\r\n    <baseline>0.2</baseline>\r\n    <alwaysOn>true</alwaysOn>\r\n    <imageTopicName>color/image_raw</imageTopicName>\r\n    <cameraInfoTopicName>color/camera_info</cameraInfoTopicName>\r\n    <depthImageTopicName>depth/image_raw</depthImageTopicName>\r\n    <depthImageCameraInfoTopicName>depth/camera_info</depthImageCameraInfoTopicName>\r\n    <pointCloudTopicName>depth/color/points</pointCloudTopicName>\r\n    <pointCloudCutoff>0.5</pointCloudCutoff>\r\n    <frameName>camera_depth_frame</frameName>\r\n  </plugin>\r\n</sensor>\n'})}),"\n",(0,i.jsx)(n.h2,{id:"4-imu-sensors",children:"4. IMU Sensors"}),"\n",(0,i.jsxs)(n.p,{children:["An ",(0,i.jsx)(n.strong,{children:"IMU (Inertial Measurement Unit)"})," sensor measures a robot's orientation, angular velocity, and linear acceleration. This data is critical for tasks like robot localization, stabilization, and control."]}),"\n",(0,i.jsxs)(n.p,{children:["In Gazebo, a simulated IMU typically uses the ",(0,i.jsx)(n.code,{children:"imu"})," sensor type."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:[(0,i.jsx)(n.code,{children:"imu"})," element"]}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"angular_velocity"}),", ",(0,i.jsx)(n.code,{children:"linear_acceleration"}),": Define noise properties."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"plugin"})}),": A ROS 2 Gazebo plugin (e.g., ",(0,i.jsx)(n.code,{children:"libgazebo_ros_imu_sensor.so"}),") to publish ",(0,i.jsx)(n.code,{children:"sensor_msgs/msg/Imu"})," messages to ROS 2 topics."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example (SDF Snippet for an IMU Sensor):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<sensor name="imu_sensor" type="imu">\r\n  <pose>0 0 0.05 0 0 0</pose>\r\n  <always_on>true</always_on>\r\n  <update_rate>100</update_rate> \x3c!-- 100 Hz --\x3e\r\n  <imu>\r\n    <angular_velocity>\r\n      <x>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.0002</stddev>\r\n          <bias_mean>0.0000075</bias_mean>\r\n          <bias_stddev>0.0000008</bias_stddev>\r\n        </noise>\r\n      </x>\r\n      \x3c!-- y and z similar --\x3e\r\n    </angular_velocity>\r\n    <linear_acceleration>\r\n      <x>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.017</stddev>\r\n          <bias_mean>0.1</bias_mean>\r\n          <bias_stddev>0.001</bias_stddev>\r\n        </noise>\r\n      </x>\r\n      \x3c!-- y and z similar --\x3e\r\n    </linear_acceleration>\r\n  </imu>\r\n  <plugin name="imu_controller" filename="libgazebo_ros_imu_sensor.so">\r\n    <ros>\r\n      <namespace>/imu</namespace>\r\n      <argument>--ros-args -r /imu:=imu/data</argument>\r\n    </ros>\r\n    <topicName>data</topicName>\r\n    <frameName>imu_link</frameName>\r\n  </plugin>\r\n</sensor>\n'})}),"\n",(0,i.jsx)(n.h2,{id:"5-reading-sensor-data-in-ros-2",children:"5. Reading Sensor Data in ROS 2"}),"\n",(0,i.jsxs)(n.p,{children:["Once your simulated sensors are configured in Gazebo and connected via ROS 2 plugins, they will publish data to specific ROS 2 topics. You can then write ROS 2 nodes (e.g., using ",(0,i.jsx)(n.code,{children:"rclpy"})," or ",(0,i.jsx)(n.code,{children:"rclcpp"}),") to subscribe to these topics and process the sensor data."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"General steps to read sensor data:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Identify the topic name and message type"}),": Use ",(0,i.jsx)(n.code,{children:"ros2 topic list"})," and ",(0,i.jsx)(n.code,{children:"ros2 topic info <topic_name>"})," to find out what topics your simulated sensors are publishing to and what message types they use."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Create a subscriber node"}),": Write a ROS 2 node that subscribes to the sensor data topic."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implement a callback function"}),": This function will be executed every time new sensor data is received. Inside the callback, you can access and process the data fields of the received message."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example Python Snippet for Subscribing to a LaserScan:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan\r\n\r\nclass LaserScanSubscriber(Node):\r\n    def __init__(self):\r\n        super().__init__('laser_subscriber')\r\n        self.subscription = self.create_subscription(\r\n            LaserScan,\r\n            '/scan',  # Topic name where LiDAR data is published\r\n            self.laser_callback,\r\n            10\r\n        )\r\n        self.get_logger().info('LaserScan Subscriber Node has started.')\r\n\r\n    def laser_callback(self, msg):\r\n        # Process the LaserScan message\r\n        # msg.header: Contains timestamp and frame_id\r\n        # msg.angle_min, msg.angle_max, msg.angle_increment: Scan angles\r\n        # msg.range_min, msg.range_max: Min/Max range values\r\n        # msg.ranges: Array of range values (distances)\r\n        # msg.intensities: Array of intensity values (if available)\r\n\r\n        self.get_logger().info(f\"Received LaserScan from frame: {msg.header.frame_id}\")\r\n        # Example: print the first and last range reading\r\n        if len(msg.ranges) > 0:\r\n            self.get_logger().info(f\"  First range: {msg.ranges[0]:.2f}m\")\r\n            self.get_logger().info(f\"  Last range: {msg.ranges[-1]:.2f}m\")\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = LaserScanSubscriber()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsxs)(n.p,{children:["This subscriber would run alongside your Gazebo simulation, receiving and processing the data generated by your simulated LiDAR sensor. The same principles apply to depth camera (",(0,i.jsx)(n.code,{children:"Image"}),", ",(0,i.jsx)(n.code,{children:"PointCloud2"}),") and IMU (",(0,i.jsx)(n.code,{children:"Imu"}),") sensor data."]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>t});var r=s(6540);const i={},a=r.createContext(i);function o(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);