"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[3511],{2163:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"module-4-vla/voice-to-action","title":"Voice-to-Action Pipeline with OpenAI Whisper","description":"1. Introduction to Voice-to-Action in Robotics","source":"@site/docs/module-4-vla/voice-to-action.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/voice-to-action","permalink":"/physical-ai-book/docs/module-4-vla/voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/RukhsarMalik/physical-ai-book/tree/main/docs/module-4-vla/voice-to-action.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"voice-to-action","title":"Voice-to-Action Pipeline with OpenAI Whisper","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"VLA and LLM-Robotics Convergence","permalink":"/physical-ai-book/docs/module-4-vla/intro"},"next":{"title":"LLMs for Robot Cognitive Planning","permalink":"/physical-ai-book/docs/module-4-vla/cognitive-planning"}}');var r=i(4848),t=i(8453);const s={id:"voice-to-action",title:"Voice-to-Action Pipeline with OpenAI Whisper",sidebar_position:2},a=void 0,c={},d=[{value:"1. Introduction to Voice-to-Action in Robotics",id:"1-introduction-to-voice-to-action-in-robotics",level:2},{value:"2. OpenAI Whisper Integration for Speech Recognition",id:"2-openai-whisper-integration-for-speech-recognition",level:2},{value:"Speech Recognition Setup",id:"speech-recognition-setup",level:3},{value:"Capturing Audio",id:"capturing-audio",level:3},{value:"3. Voice Command Processing with Whisper API",id:"3-voice-command-processing-with-whisper-api",level:2},{value:"4. Converting Voice Commands to Robot Actions",id:"4-converting-voice-commands-to-robot-actions",level:2}];function l(e){const n={code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"1-introduction-to-voice-to-action-in-robotics",children:"1. Introduction to Voice-to-Action in Robotics"}),"\n",(0,r.jsxs)(n.p,{children:["In the realm of Vision-Language-Action (VLA) robotics, enabling robots to understand and respond to human voice commands is a pivotal capability. The ",(0,r.jsx)(n.strong,{children:"Voice-to-Action pipeline"})," describes the sequence of steps that transforms spoken language into executable robot behaviors. This pipeline typically involves speech recognition, natural language understanding (NLU), and the translation of understood commands into robot-specific actions."]}),"\n",(0,r.jsx)(n.p,{children:"The goal is to create a natural and intuitive interface where humans can communicate with robots using their voice, bypassing the need for complex programming or graphical user interfaces for common tasks."}),"\n",(0,r.jsx)(n.h2,{id:"2-openai-whisper-integration-for-speech-recognition",children:"2. OpenAI Whisper Integration for Speech Recognition"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"OpenAI Whisper"})," is a general-purpose, open-source automatic speech recognition (ASR) system. It has been trained on a massive dataset of diverse audio and performs exceptionally well across various languages, accents, and noisy environments. Its high accuracy and robust performance make it an excellent choice for the speech recognition component of a voice-to-action pipeline in robotics."]}),"\n",(0,r.jsx)(n.h3,{id:"speech-recognition-setup",children:"Speech Recognition Setup"}),"\n",(0,r.jsx)(n.p,{children:"To use OpenAI Whisper, you typically interact with it via the OpenAI API (for cloud-based transcription) or by running a local model (if you have sufficient computational resources and prefer offline processing). For robotic applications, the API is often preferred for its ease of use and high accuracy, provided internet connectivity is stable."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Installation (OpenAI Python Library):"}),"\r\nFirst, ensure you have the OpenAI Python library installed:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install openai\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"API Key Configuration:"}),"\r\nAs emphasized previously, manage your OpenAI API key securely. The recommended method for development is via an environment variable."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Set in your terminal (Linux/macOS)\r\nexport OPENAI_API_KEY="YOUR_API_KEY_HERE"\r\n\r\n# For Windows PowerShell\r\n# $env:OPENAI_API_KEY="YOUR_API_KEY_HERE"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"capturing-audio",children:"Capturing Audio"}),"\n",(0,r.jsx)(n.p,{children:"Before Whisper can transcribe, the robot needs to capture audio. This typically involves:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Microphone Hardware"}),": A USB microphone or an array of microphones integrated into the robot."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio Capture Software"}),": Libraries like ",(0,r.jsx)(n.code,{children:"pyaudio"})," in Python can interface with microphones to record audio streams. In ROS 2, dedicated audio packages (e.g., ",(0,r.jsx)(n.code,{children:"ros2_audio_driver"}),") might be used to publish audio data to ROS 2 topics."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"3-voice-command-processing-with-whisper-api",children:"3. Voice Command Processing with Whisper API"}),"\n",(0,r.jsx)(n.p,{children:"Once audio is captured, it needs to be processed by Whisper. The Whisper API expects an audio file."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Conceptual Python Code Example using Whisper API:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import openai\r\nimport os\r\nimport io\r\nimport wave # For handling WAV files (common format)\r\n\r\n# --- Configuration ---\r\n# Ensure OPENAI_API_KEY is set as an environment variable\r\nopenai.api_key = os.getenv("OPENAI_API_KEY")\r\n\r\n# --- Function to simulate audio capture (replace with actual microphone input) ---\r\ndef simulate_audio_capture(text_to_say="Hello robot, navigate to the kitchen."):\r\n    """\r\n    This function would typically record audio from a microphone.\r\n    For demonstration, we\'ll create a dummy WAV file.\r\n    In a real robot, you\'d record actual speech.\r\n    """\r\n    # This is a placeholder. A real implementation would use pyaudio or a ROS audio driver\r\n    # to record from the microphone and save to a temporary WAV file.\r\n    dummy_wav_path = "/tmp/recorded_command.wav"\r\n    with wave.open(dummy_wav_path, \'wb\') as wf:\r\n        wf.setnchannels(1) # mono\r\n        wf.setsampwidth(2) # 16-bit\r\n        wf.setframerate(16000) # 16kHz\r\n        # Write dummy audio data (e.g., silence or synthetic speech)\r\n        wf.writeframes(b\'\\x00\\x00\' * 16000) # 1 second of "silence"\r\n    print(f"Simulated audio recorded to {dummy_wav_path}")\r\n    return dummy_wav_path\r\n\r\n# --- Function to transcribe audio using OpenAI Whisper API ---\r\ndef transcribe_audio_with_whisper(audio_file_path):\r\n    try:\r\n        with open(audio_file_path, "rb") as audio_file:\r\n            print(f"Sending {audio_file_path} to Whisper API...")\r\n            # For older OpenAI library versions:\r\n            # transcript = openai.Audio.transcribe("whisper-1", audio_file)\r\n            # For newer OpenAI library versions (recommended):\r\n            transcript = openai.audio.transcriptions.create(\r\n                model="whisper-1",\r\n                file=audio_file\r\n            )\r\n        return transcript.text\r\n    except openai.APIConnectionError as e:\r\n        print(f"OpenAI API Connection Error: {e}")\r\n        print("Please check your internet connection and API key.")\r\n        return None\r\n    except openai.APIStatusError as e:\r\n        print(f"OpenAI API Status Error: {e.status_code} - {e.response}")\r\n        print("Please check your API key and request parameters.")\r\n        return None\r\n    except Exception as e:\r\n        print(f"An unexpected error occurred: {e}")\r\n        return None\r\n\r\n# --- Main voice-to-action processing function ---\r\ndef process_voice_command():\r\n    print("Robot is listening for your command...")\r\n    audio_path = simulate_audio_capture() # Simulate recording\r\n\r\n    if audio_path:\r\n        transcribed_text = transcribe_audio_with_whisper(audio_path)\r\n        if transcribed_text:\r\n            print(f"Transcribed Text: \'{transcribed_text}\'")\r\n            # --- Next Step: Natural Language Understanding (NLU) ---\r\n            # This is where an LLM (e.g., GPT) would take the transcribed text\r\n            # and extract intent, entities, and parameters for robot actions.\r\n            print(f"Sending \'{transcribed_text}\' for NLU and action planning...")\r\n            return transcribed_text\r\n    return None\r\n\r\nif __name__ == "__main__":\r\n    if not os.getenv("OPENAI_API_KEY"):\r\n        print("Error: OPENAI_API_KEY environment variable not set.")\r\n        print("Please set it before running: export OPENAI_API_KEY=\'YOUR_KEY\'")\r\n    else:\r\n        processed_command = process_voice_command()\r\n        if processed_command:\r\n            print("Voice command pipeline completed for transcription stage.")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"4-converting-voice-commands-to-robot-actions",children:"4. Converting Voice Commands to Robot Actions"}),"\n",(0,r.jsx)(n.p,{children:"Once Whisper has transcribed the spoken command into text, the next critical step is to convert this text into a format that the robot can understand and execute. This usually involves two sub-steps:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),": An NLU component (often another LLM or a specialized NLP model) analyzes the transcribed text to identify the user's ",(0,r.jsx)(n.strong,{children:"intent"}),' (e.g., "navigate," "pick up," "find") and extract relevant ',(0,r.jsx)(n.strong,{children:"entities"})," and ",(0,r.jsx)(n.strong,{children:"parameters"}),' (e.g., "kitchen," "cup," "red box").']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Action Mapping"}),": The extracted intent and parameters are then mapped to specific robot actions. In a ROS 2 system, these actions typically correspond to:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Actions"}),": For long-running, goal-oriented tasks (e.g., ",(0,r.jsx)(n.code,{children:"NavigateToPose"}),", ",(0,r.jsx)(n.code,{children:"PickObject"}),")."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Services"}),": For single request-response operations (e.g., ",(0,r.jsx)(n.code,{children:"GetBatteryStatus"}),")."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Topics"}),": For publishing simple commands (e.g., velocity commands to a mobile base)."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'Example: Voice "Move forward" \u2192 ROS 2 command'})}),"\n",(0,r.jsx)(n.p,{children:'Let\'s imagine a user says, "Robot, move forward by two meters."'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Whisper"}),': Transcribes "Robot, move forward by two meters."']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"NLU (LLM)"}),": Analyzes the text, extracts intent: ",(0,r.jsx)(n.code,{children:"navigate"}),", parameter: ",(0,r.jsx)(n.code,{children:"distance=2m"}),", direction: ",(0,r.jsx)(n.code,{children:"forward"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action Mapping"}),": Translates this into a ROS 2 command. For a mobile robot, this might involve:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Sending a goal to a ",(0,r.jsx)(n.code,{children:"NavigateToPose"})," action client, with the target pose being 2 meters directly in front of the robot."]}),"\n",(0,r.jsxs)(n.li,{children:["Alternatively, publishing a ",(0,r.jsx)(n.code,{children:"geometry_msgs/msg/Twist"})," message to a ",(0,r.jsx)(n.code,{children:"/cmd_vel"})," topic for a short duration, with linear ",(0,r.jsx)(n.code,{children:"x"})," velocity set to a positive value."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This entire sequence, from a human's spoken word to a robot's physical movement, forms the Voice-to-Action pipeline, demonstrating a fundamental aspect of VLA."})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var o=i(6540);const r={},t=o.createContext(r);function s(e){const n=o.useContext(t);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(t.Provider,{value:n},e.children)}}}]);