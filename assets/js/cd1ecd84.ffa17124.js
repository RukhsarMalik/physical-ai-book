"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[5757],{8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}},9357:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla/capstone-project","title":"Autonomous Humanoid Capstone Project","description":"1. Project Overview: Bringing VLA to Life","source":"@site/docs/module-4-vla/capstone-project.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/capstone-project","permalink":"/physical-ai-book/docs/module-4-vla/capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/RukhsarMalik/physical-ai-book/tree/main/docs/module-4-vla/capstone-project.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"capstone-project","title":"Autonomous Humanoid Capstone Project","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"LLMs for Robot Cognitive Planning","permalink":"/physical-ai-book/docs/module-4-vla/cognitive-planning"}}');var o=i(4848),s=i(8453);const r={id:"capstone-project",title:"Autonomous Humanoid Capstone Project",sidebar_position:4},a=void 0,l={},c=[{value:"1. Project Overview: Bringing VLA to Life",id:"1-project-overview-bringing-vla-to-life",level:2},{value:"2. Complete Workflow: Voice Command to Physical Action",id:"2-complete-workflow-voice-command-to-physical-action",level:2},{value:"Step 1: Receive Voice Command (OpenAI Whisper Integration)",id:"step-1-receive-voice-command-openai-whisper-integration",level:3},{value:"Step 2: Plan Path and Decompose Task (LLM-based Cognitive Planning)",id:"step-2-plan-path-and-decompose-task-llm-based-cognitive-planning",level:3},{value:"Step 3: Execute Navigation (Nav2 Integration)",id:"step-3-execute-navigation-nav2-integration",level:3},{value:"Step 4: Identify Object (Computer Vision / Perception)",id:"step-4-identify-object-computer-vision--perception",level:3},{value:"Step 5: Manipulate Object (Grasping and Placement)",id:"step-5-manipulate-object-grasping-and-placement",level:3},{value:"3. System Architecture Diagram Description",id:"3-system-architecture-diagram-description",level:2},{value:"4. Implementation Roadmap and Technologies Integration",id:"4-implementation-roadmap-and-technologies-integration",level:2}];function d(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"1-project-overview-bringing-vla-to-life",children:"1. Project Overview: Bringing VLA to Life"}),"\n",(0,o.jsxs)(n.p,{children:["This capstone project outlines a conceptual framework for building an ",(0,o.jsx)(n.strong,{children:"autonomous humanoid robot"})," that integrates the Vision-Language-Action (VLA) principles learned in this module. The goal is to enable the humanoid to understand high-level natural language commands, perceive its environment, plan complex actions, and execute them physically. This project serves as an end-to-end example of how LLMs, speech recognition, computer vision, and ROS 2 can converge to create truly intelligent robotic systems."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Target Capabilities"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice Command Interface"}),": Understand human spoken instructions."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cognitive Planning"}),": Decompose abstract commands into executable robot actions."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Autonomous Navigation"}),": Move to specified locations while avoiding obstacles."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Identification"}),": Recognize and locate specific objects in the environment."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation"}),": Grasp and interact with objects."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Why a Humanoid?"}),": Humanoid robots are the ultimate platform for VLA, as their form factor and manipulation capabilities are designed for human-centric environments and tasks, making the natural language interface particularly powerful."]}),"\n",(0,o.jsx)(n.h2,{id:"2-complete-workflow-voice-command-to-physical-action",children:"2. Complete Workflow: Voice Command to Physical Action"}),"\n",(0,o.jsx)(n.p,{children:"The full workflow for this autonomous humanoid capstone project would proceed as follows:"}),"\n",(0,o.jsx)(n.h3,{id:"step-1-receive-voice-command-openai-whisper-integration",children:"Step 1: Receive Voice Command (OpenAI Whisper Integration)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Input"}),': Human spoken instruction (e.g., "Robot, please bring me the red mug from the kitchen table.").']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Process"}),":","\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"The humanoid's microphone array captures audio."}),"\n",(0,o.jsxs)(n.li,{children:["This audio stream is sent to a dedicated ",(0,o.jsx)(n.strong,{children:"speech recognition node"})," (e.g., a ROS 2 node running an OpenAI Whisper client or integrating with the Whisper API)."]}),"\n",(0,o.jsx)(n.li,{children:"Whisper transcribes the audio into a textual string."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Output"}),": Transcribed text of the human command."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"step-2-plan-path-and-decompose-task-llm-based-cognitive-planning",children:"Step 2: Plan Path and Decompose Task (LLM-based Cognitive Planning)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Input"}),': Transcribed text from Whisper (e.g., "bring me the red mug from the kitchen table").']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Process"}),":","\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["The transcribed text is fed to a ",(0,o.jsx)(n.strong,{children:"cognitive planning node"})," (e.g., a Python ROS 2 node that acts as an LLM client, using GPT-4)."]}),"\n",(0,o.jsx)(n.li,{children:"The LLM, through careful prompt engineering (including context about robot capabilities and available tools), interprets the high-level command."}),"\n",(0,o.jsx)(n.li,{children:'It then generates a detailed, step-by-step plan consisting of discrete, robot-executable actions (e.g., "Navigate to kitchen," "Identify red mug on table," "Grasp mug," "Navigate to human").'}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Output"}),": A structured sequence of high-level ROS 2 actions (e.g., a JSON or YAML list of action goals)."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"step-3-execute-navigation-nav2-integration",children:"Step 3: Execute Navigation (Nav2 Integration)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Input"}),": A navigation action goal from the cognitive planner (e.g., ",(0,o.jsx)(n.code,{children:"NavigateToPose(target_location='kitchen')"}),")."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Process"}),":","\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["The humanoid's ",(0,o.jsx)(n.strong,{children:"navigation stack"})," (e.g., Nav2) receives the high-level navigation goal."]}),"\n",(0,o.jsx)(n.li,{children:"Nav2 leverages the robot's sensors (LiDAR, cameras, IMU) and map to plan a global path and continuously execute local motions, avoiding obstacles."}),"\n",(0,o.jsx)(n.li,{children:"The humanoid's locomotion controller translates Nav2's velocity commands into stable bipedal walking motions."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Output"}),": Robot successfully reaches the target location."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"step-4-identify-object-computer-vision--perception",children:"Step 4: Identify Object (Computer Vision / Perception)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Input"}),': Robot\'s current visual sensor data (e.g., camera feed), and an object identification task from the planner (e.g., "Identify red mug on table").']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Process"}),":","\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["A ",(0,o.jsx)(n.strong,{children:"perception node"})," (e.g., a ROS 2 node leveraging an Isaac ROS accelerated object detection model or a custom YOLO/DETR model) processes the camera feed."]}),"\n",(0,o.jsx)(n.li,{children:"It identifies the specified object (red mug) within the current scene and estimates its 3D pose relative to the robot."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Output"}),": 3D pose of the identified object."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"step-5-manipulate-object-grasping-and-placement",children:"Step 5: Manipulate Object (Grasping and Placement)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Input"}),": Object's 3D pose, and a manipulation task from the planner (e.g., ",(0,o.jsx)(n.code,{children:"GraspObject(target_object_pose)"}),")."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Process"}),":","\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["A ",(0,o.jsx)(n.strong,{children:"manipulation node"})," (e.g., a ROS 2 node integrating MoveIt 2 for motion planning and a dedicated grasping controller) receives the object's pose."]}),"\n",(0,o.jsx)(n.li,{children:"It plans a collision-free trajectory for the humanoid's arm(s) to reach and grasp the object."}),"\n",(0,o.jsx)(n.li,{children:"The humanoid's end-effector (hand/gripper) executes the grasp."}),"\n",(0,o.jsx)(n.li,{children:"Subsequent actions might involve navigating to a drop-off location and releasing the object."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Output"}),": Object successfully grasped and potentially placed."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"3-system-architecture-diagram-description",children:"3. System Architecture Diagram Description"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.em,{children:(0,o.jsx)(n.strong,{children:"Diagram Description:"})}),"\r\n*A high-level block diagram illustrating the end-to-end VLA workflow for the autonomous humanoid. It would show:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"A human speaking to the robot."}),"\n",(0,o.jsx)(n.li,{children:'An "Audio Input" block leading to an "OpenAI Whisper (Speech-to-Text)" block.'}),"\n",(0,o.jsx)(n.li,{children:'The output of Whisper flowing into a "Cognitive Planning (LLM)" block.'}),"\n",(0,o.jsx)(n.li,{children:'The LLM output (Action Plan) feeding into a "ROS 2 Executive" block.'}),"\n",(0,o.jsxs)(n.li,{children:['The "ROS 2 Executive" connecting to:',"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'"Navigation Stack (Nav2)" (which also takes input from "Sensors" and a "Map").'}),"\n",(0,o.jsx)(n.li,{children:'"Perception System (Computer Vision)" (taking input from "Sensors").'}),"\n",(0,o.jsx)(n.li,{children:'"Manipulation & Control" (connected to "Actuators").'}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.li,{children:'Feedback loops would go from "Navigation," "Perception," and "Manipulation" back to the "ROS 2 Executive" and potentially to the "Cognitive Planning (LLM)" for replanning or status updates.'}),"\n",(0,o.jsx)(n.li,{children:'The "Sensors" block would feed into "Perception" and "Navigation."'}),"\n",(0,o.jsx)(n.li,{children:'The "Actuators" block would be controlled by "Manipulation & Control."*'}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"4-implementation-roadmap-and-technologies-integration",children:"4. Implementation Roadmap and Technologies Integration"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Implementation Stages:"})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Environment Setup"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Set up ROS 2 on a compatible platform (e.g., Ubuntu)."}),"\n",(0,o.jsxs)(n.li,{children:["Configure Python environment with ",(0,o.jsx)(n.code,{children:"openai"})," library."]}),"\n",(0,o.jsx)(n.li,{children:"(Optional but Recommended) Set up a simulation environment (Isaac Sim, Gazebo) for virtual testing."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech Recognition Integration"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Develop a ROS 2 node to capture audio from a microphone."}),"\n",(0,o.jsx)(n.li,{children:"Implement an OpenAI Whisper client within a ROS 2 node to transcribe audio."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"LLM Planning Integration"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Develop a ROS 2 node that interfaces with the OpenAI GPT API."}),"\n",(0,o.jsx)(n.li,{children:"Design effective prompts for converting natural language commands into structured robot plans."}),"\n",(0,o.jsx)(n.li,{children:"Implement logic to parse LLM output into ROS 2 action sequences."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robotics Stack Integration"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Integrate Nav2 for autonomous navigation (if using a mobile base)."}),"\n",(0,o.jsx)(n.li,{children:"Develop or integrate perception nodes for object identification (e.g., using existing ROS 2 computer vision packages or Isaac ROS)."}),"\n",(0,o.jsx)(n.li,{children:"Integrate MoveIt 2 for manipulation planning and execution with a humanoid arm."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Humanoid Control"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement or integrate low-level controllers for bipedal locomotion, balance, and whole-body control."}),"\n",(0,o.jsx)(n.li,{children:"Map ROS 2 action commands to these low-level controllers."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Capstone Workflow Orchestration"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Create a top-level ROS 2 launch file to bring up all VLA components."}),"\n",(0,o.jsx)(n.li,{children:"Develop a state machine or behavior tree to manage the overall workflow."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Technologies Integration"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Operating System"}),": Linux (Ubuntu 22.04 LTS recommended)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robotics Framework"}),": ROS 2 (Humble, Iron, or newer)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech-to-Text"}),": OpenAI Whisper API (Python client)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cognitive Planning"}),": OpenAI GPT-4 API (Python client)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation"}),": Nav2 (ROS 2 package)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception"}),": ROS 2 computer vision packages, potentially Isaac ROS accelerated components"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation"}),": MoveIt 2 (ROS 2 package)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simulation (for testing)"}),": Gazebo or Isaac Sim"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Humanoid Control"}),": Specific libraries/packages for humanoid locomotion (e.g., ",(0,o.jsx)(n.code,{children:"humanoid_control"})," ROS 2 package, custom controllers)"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);