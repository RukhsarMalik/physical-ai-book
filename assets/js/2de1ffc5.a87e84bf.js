"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[1963],{6568:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-vla/cognitive-planning","title":"LLMs for Robot Cognitive Planning","description":"1. Introduction to Cognitive Planning with LLMs","source":"@site/docs/module-4-vla/cognitive-planning.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/cognitive-planning","permalink":"/physical-ai-book/docs/module-4-vla/cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/RukhsarMalik/physical-ai-book/tree/main/docs/module-4-vla/cognitive-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"cognitive-planning","title":"LLMs for Robot Cognitive Planning","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action Pipeline with OpenAI Whisper","permalink":"/physical-ai-book/docs/module-4-vla/voice-to-action"},"next":{"title":"Autonomous Humanoid Capstone Project","permalink":"/physical-ai-book/docs/module-4-vla/capstone-project"}}');var i=t(4848),r=t(8453);const a={id:"cognitive-planning",title:"LLMs for Robot Cognitive Planning",sidebar_position:3},s=void 0,l={},c=[{value:"1. Introduction to Cognitive Planning with LLMs",id:"1-introduction-to-cognitive-planning-with-llms",level:2},{value:"2. Natural Language to Action Sequences",id:"2-natural-language-to-action-sequences",level:2},{value:"3. LLM Prompt Engineering for Robotics",id:"3-llm-prompt-engineering-for-robotics",level:2},{value:"4. Code Example: Basic LLM Integration with Robotics",id:"4-code-example-basic-llm-integration-with-robotics",level:2}];function d(e){const n={code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"1-introduction-to-cognitive-planning-with-llms",children:"1. Introduction to Cognitive Planning with LLMs"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Cognitive planning"})," in robotics refers to the ability of a robot to reason about its goals, environment, and capabilities to generate a sequence of actions that achieves a desired outcome. Traditionally, this has involved complex symbolic AI, state-space search, or hand-coded finite state machines. The emergence of ",(0,i.jsx)(n.strong,{children:"Large Language Models (LLMs)"})," has opened up a new paradigm for cognitive planning, enabling robots to leverage the vast world knowledge and reasoning capabilities embedded within these models."]}),"\n",(0,i.jsx)(n.p,{children:'LLMs can bridge the gap between high-level, abstract human language instructions and the low-level, concrete actions a robot can execute. They act as a "brain" that can understand intent, contextualize commands, and propose executable plans.'}),"\n",(0,i.jsx)(n.h2,{id:"2-natural-language-to-action-sequences",children:"2. Natural Language to Action Sequences"}),"\n",(0,i.jsx)(n.p,{children:"One of the most powerful applications of LLMs in robotics is their ability to translate vague or complex natural language instructions into a sequence of more specific, robot-executable actions."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"The process typically involves:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Human Instruction"}),': A user provides a high-level command (e.g., "Go clean the table").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLM Interpretation"}),": The LLM processes this command, leveraging its knowledge of the world to understand the intent (",(0,i.jsx)(n.code,{children:"clean"}),") and relevant objects/locations (",(0,i.jsx)(n.code,{children:"table"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Decomposition"}),": The LLM decomposes the high-level command into a series of smaller, more manageable sub-goals or primitive actions that the robot can perform. This might involve generating a plan in natural language first, then translating those natural language steps into robot-specific commands."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robot-Executable Format"}),": The decomposed actions are translated into a format directly usable by the robot's control system, often ROS 2 actions, services, or topics."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:'Example: "Clean the room" \u2192 ROS 2 Action Plan'})}),"\n",(0,i.jsx)(n.p,{children:'Let\'s consider a complex instruction like "Clean the room." An LLM, given appropriate context and prompts, could generate a plan similar to this:'}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Initial Human Command"}),': "Robot, please clean the living room."']}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"LLM-Generated High-Level Plan (Natural Language)"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Navigate to the living room."}),"\n",(0,i.jsx)(n.li,{children:"Identify any trash or clutter on the floor or table."}),"\n",(0,i.jsx)(n.li,{children:"Pick up each piece of trash."}),"\n",(0,i.jsx)(n.li,{children:"Place trash in the waste bin."}),"\n",(0,i.jsx)(n.li,{children:"Sweep the floor."}),"\n",(0,i.jsx)(n.li,{children:"Return to the charging station."}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Translation to ROS 2 Action Plan (Conceptual)"}),":\r\nThis natural language plan would then be translated into a sequence of ROS 2 actions or service calls. Each high-level step corresponds to one or more robot capabilities:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"ROS2 Action: navigate_to_pose (target: living_room_coords)"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"ROS2 Service: perceive_scene (area_of_interest: living_room)"})," -> returns ",(0,i.jsx)(n.code,{children:"list_of_objects_to_pick"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"LOOP through list_of_objects_to_pick:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"ROS2 Action: pick_object (object_id: current_object)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"ROS2 Action: place_object (target_location: waste_bin_coords)"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"ROS2 Action: sweep_area (area: living_room_floor)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"ROS2 Action: navigate_to_pose (target: charging_station_coords)"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This detailed breakdown shows how an LLM can convert a single human command into a robust, executable plan for a robot."}),"\n",(0,i.jsx)(n.h2,{id:"3-llm-prompt-engineering-for-robotics",children:"3. LLM Prompt Engineering for Robotics"}),"\n",(0,i.jsxs)(n.p,{children:["The effectiveness of an LLM in generating robot plans heavily relies on ",(0,i.jsx)(n.strong,{children:"prompt engineering"}),". This involves carefully crafting the input given to the LLM to elicit the desired output."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key considerations for prompt engineering in robotics:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Role-Playing"}),': Assigning the LLM a specific role (e.g., "You are a robotic task planner...").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context Provision"}),": Providing the LLM with information about the robot's capabilities, its environment, available tools, and current state."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output Format"}),": Specifying the desired format for the plan (e.g., bullet points, JSON, or a list of ROS 2 action calls)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Few-Shot Examples"}),": Giving the LLM a few examples of input commands and their corresponding robot plans to guide its understanding."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Constraints and Safety"}),": Explicitly instructing the LLM about safety protocols, forbidden actions, or physical limitations of the robot."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"4-code-example-basic-llm-integration-with-robotics",children:"4. Code Example: Basic LLM Integration with Robotics"}),"\n",(0,i.jsx)(n.p,{children:"This conceptual Python code snippet demonstrates how you might integrate a GPT model to generate a high-level robot plan from a natural language command."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import openai\r\nimport os\r\n\r\n# --- Configuration ---\r\n# Ensure OPENAI_API_KEY is set as an environment variable\r\nopenai.api_key = os.getenv("OPENAI_API_KEY")\r\n\r\n# --- Function to get robot plan from LLM ---\r\ndef generate_robot_plan_with_llm(user_command: str) -> str:\r\n    """\r\n    Generates a high-level robot action plan from a user command using an LLM.\r\n    """\r\n    if not openai.api_key:\r\n        return "Error: OpenAI API key not set."\r\n\r\n    system_prompt = """You are a robotic task planner. Your goal is to convert high-level natural language instructions into a sequence of discrete, high-level robot actions. Focus on logical steps. Each step should be a clear, simple robot action. Do not generate code directly, just the sequence of natural language actions.\r\n\r\nAvailable high-level robot capabilities:\r\n- NAVIGATE_TO(location: str)\r\n- PICK_UP(object: str)\r\n- PLACE_AT(location: str)\r\n- SWEEP(area: str)\r\n- IDENTIFY(object: str, area: str)\r\n\r\nExample:\r\nUser command: \\"Go to the kitchen and grab a cup.\\"\r\nPlan:\r\n1. NAVIGATE_TO(kitchen)\r\n2. IDENTIFY(cup, kitchen)\r\n3. PICK_UP(cup)\r\n4. NAVIGATE_TO(start_location) # assuming robot returns to where it was commanded\r\n"""\r\n\r\n    user_prompt = f"User command: \\"{user_command}\\"\\nPlan:"\r\n\r\n    try:\r\n        response = openai.chat.completions.create(\r\n            model="gpt-4",  # Use a capable LLM\r\n            messages=[\r\n                {"role": "system", "content": system_prompt},\r\n                {"role": "user", "content": user_prompt}\r\n            ],\r\n            temperature=0.2, # Lower temperature for more deterministic planning\r\n            max_tokens=200\r\n        )\r\n        plan_text = response.choices[0].message.content\r\n        return plan_text\r\n    except openai.APIConnectionError as e:\r\n        return f"OpenAI API Connection Error: {e}. Check internet and API key."\r\n    except openai.APIStatusError as e:\r\n        return f"OpenAI API Status Error: {e.status_code} - {e.response.json()}. Check API key/model."\r\n    except Exception as e:\r\n        return f"An unexpected error occurred: {e}"\r\n\r\n# --- Main execution example ---\r\nif __name__ == "__main__":\r\n    if not os.getenv("OPENAI_API_KEY"):\r\n        print("Error: OPENAI_API_KEY environment variable not set.")\r\n        print("Please set it before running: export OPENAI_API_KEY=\'YOUR_KEY\'")\r\n    else:\r\n        command = "Clean the living room table and then come back."\r\n        print(f"User command: {command}")\r\n        plan = generate_robot_plan_with_llm(command)\r\n        print("\\n--- LLM-Generated Plan ---")\r\n        print(plan)\r\n\r\n        command_2 = "Find my phone on the sofa."\r\n        print(f"\\nUser command: {command_2}")\r\n        plan_2 = generate_robot_plan_with_llm(command_2)\r\n        print("\\n--- LLM-Generated Plan ---")\r\n        print(plan_2)\n'})}),"\n",(0,i.jsx)(n.p,{children:"This example shows how an LLM can be prompted to generate a structured plan. The next step would be to parse this plan and map it to actual ROS 2 action goals or service calls, enabling the robot to execute the high-level instruction."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var o=t(6540);const i={},r=o.createContext(i);function a(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);